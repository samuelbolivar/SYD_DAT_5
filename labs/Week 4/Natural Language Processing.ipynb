{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science with Python \n",
    "## General Assembly\n",
    "## Natural Language Processing (NLP)\n",
    "\n",
    "Make sure you have installed nltk and downloaded the following copora:\n",
    "\n",
    "* punkt\n",
    "* gutenberg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 1\n",
    "\n",
    "###Tokenization\n",
    "\n",
    "What:  Separate text into units such as sentences or words\n",
    "\n",
    "Why:   Gives structure to previously unstructured text\n",
    "\n",
    "Notes: Relatively easy with English language text, not easy with some languages\n",
    "\n",
    "\n",
    "\"corpus\" = collection of documents\n",
    "\n",
    "\"corpora\" = plural form of corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'austen-emma.txt',\n",
       " u'austen-persuasion.txt',\n",
       " u'austen-sense.txt',\n",
       " u'bible-kjv.txt',\n",
       " u'blake-poems.txt',\n",
       " u'bryant-stories.txt',\n",
       " u'burgess-busterbrown.txt',\n",
       " u'carroll-alice.txt',\n",
       " u'chesterton-ball.txt',\n",
       " u'chesterton-brown.txt',\n",
       " u'chesterton-thursday.txt',\n",
       " u'edgeworth-parents.txt',\n",
       " u'melville-moby_dick.txt',\n",
       " u'milton-paradise.txt',\n",
       " u'shakespeare-caesar.txt',\n",
       " u'shakespeare-hamlet.txt',\n",
       " u'shakespeare-macbeth.txt',\n",
       " u'whitman-leaves.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the NLTK library, and use ntlk.corpus.gutenberg.fileids() to\n",
    "# find the filenames for Jane Austen's Emma and Lewis Carrol's Alice in \n",
    "# Wonderland\n",
    "import nltk\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Break these novels up into sentences. Put these sentence lists into\n",
    "# a list so that you can use it later\n",
    "emma_sentences = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
    "alice_sentences = nltk.corpus.gutenberg.sents('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1703"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of sentences in each novel.\n",
    "len(emma_sentences)\n",
    "len(alice_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'[', u'Alice', u\"'\", u's', u'Adventures', u'in', u'Wonderland', u'by', u'Lewis', u'Carroll', u'1865', u']'], [u'CHAPTER', u'I', u'.'], ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Break each sentence up into words. You will end up with a \n",
    "# list of lists of words for Emma and another one for Alice in\n",
    "# Wonderland\n",
    "\n",
    "emma_sentences\n",
    "alice_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the number of words in each sentence\n",
    "words_per_sentence_emma = [len(x) for x in emma_sentences]\n",
    "words_per_sentence_alice = [len(x) for x in alice_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which novel has more average words per sentence?\n",
    "# Given their target audience, is this what you would expect?\n",
    "\n",
    "avg_words_per_sentence_emma = sum(words_per_sentence_emma)/len(words_per_sentence_emma)\n",
    "avg_words_per_sentence_emma\n",
    "\n",
    "avg_words_per_sentence_alice = sum(words_per_sentence_alice)/len(words_per_sentence_alice)\n",
    "avg_words_per_sentence_alice\n",
    "\n",
    "#Alice has less average words per sentence so it is expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a flat list (i.e. not a list of lists) of words in\n",
    "# the two novels\n",
    "emma_words = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "alice_words = nltk.corpus.gutenberg.words('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each novel, construct a set of all the distinct words used\n",
    "emma_distinct_words = set(emma_words)\n",
    "alice_distinct_words = set(alice_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08841981823512167"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the lexical diversity of each novel (distinct words / word count)\n",
    "\n",
    "lexical_diversity_emma = (len(emma_distinct_words)*1.0)/len(emma_words)\n",
    "lexical_diversity_emma\n",
    "\n",
    "lexical_diversity_alice = (len(alice_distinct_words)*1.0)/len(alice_words)\n",
    "lexical_diversity_alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (Optional, only for the very keen)\n",
    "# Repeat the above analysis for all the Gutenberg samples\n",
    "# Create a dataframe with the names of the novels, when they were written,\n",
    "# whether they were for children, the lexical diversity and the average sentence length.\n",
    "# Can you use logistic regression to predict the audience, based on the content?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make nltk.Text objects from the two novels\n",
    "emma_text_obj = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
    "alice_text_obj = nltk.Text(nltk.corpus.gutenberg.words('carroll-alice.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 192 matches:\n",
      "marriage . A worthy employment for a young lady ' s mind ! But if , which I rat\n",
      "ice .\" \" Mr . Elton is a very pretty young man , to be sure , and a very good y\n",
      "g man , to be sure , and a very good young man , and I have a great regard for \n",
      "hildren of their own , nor any other young creature of equal kindred to care fo\n",
      "is fond report of him as a very fine young man had made Highbury feel a sort of\n",
      "formed a very favourable idea of the young man ; and such a pleasing attention \n",
      " . Knightley ; and by Mr . Elton , a young man living alone without liking it ,\n",
      "ee of popularity for a woman neither young , handsome , rich , nor married . Mi\n",
      "nciples and new systems -- and where young ladies for enormous pay might be scr\n",
      "was no wonder that a train of twenty young couple now walked after her to churc\n",
      " a long visit in the country to some young ladies who had been at school there \n",
      " Harriet Smith ' s being exactly the young friend she wanted -- exactly the som\n",
      "was a single man ; that there was no young Mrs . Martin , no wife in the case ;\n",
      "hout having any idea of his name . A young farmer , whether on horseback or on \n",
      "oubt of his being a very respectable young man . I know , indeed , that he is s\n",
      "ly four - and - twenty . That is too young to settle . His mother is perfectly \n",
      "if he could meet with a good sort of young woman in the same rank as his own , \n",
      "w no alarming symptoms of love . The young man had been the first admirer , but\n",
      "neat , and he looked like a sensible young man , but his person had no other ad\n",
      "n life seem to allow it ; but if any young man were to set about copying him , \n",
      "erable . On the contrary , I think a young man might be very safely recommended\n",
      "son fixed on by Emma for driving the young farmer out of Harriet ' s head . She\n",
      "oured , well - meaning , respectable young man , without any deficiency of usef\n",
      " . And he was really a very pleasing young man , a young man whom any woman not\n",
      "really a very pleasing young man , a young man whom any woman not fastidious mi\n",
      "Displaying 5 of 5 matches:\n",
      " ' Hold your tongue , Ma !' said the young Crab , a little snappishly . ' You '\n",
      " You are old , Father William ,' the young man said , ' And your hair has becom\n",
      "m getting tired of this . I vote the young lady tells us a story .' ' I ' m afr\n",
      " !' said the Queen , ' and take this young lady to see the Mock Turtle , and to\n",
      "ars , but said nothing . ' This here young lady ,' said the Gryphon , ' she wan\n"
     ]
    }
   ],
   "source": [
    "# Does Jane Austen ever mention the word 'young' in Emma? What about Lewis Carroll?\n",
    "emma_text_obj.concordance('young')\n",
    "alice_text_obj.concordance('young')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accomplished_woman worthy_man the_farmer the_man unexceptionable_man\n",
      "a_man so_as amiable_man pretty_woman the_are no_mrs too_: pert_lawyer\n",
      "of_person alarming_man ,_cox a_woman too_; the_woman of_men\n",
      "the_man here_lady the_crab the_lady this_lady\n"
     ]
    }
   ],
   "source": [
    "# What are the common contexts for these words?\n",
    "emma_text_obj.common_contexts(['young'])\n",
    "alice_text_obj.common_contexts(['young'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEZCAYAAACervI0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDyUlFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c493ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Where does the word 'cat' appear in Alice and Wonderland?\n",
    "\n",
    "#alice_text_obj.similar('cat')\n",
    "#alice_text_obj.concordance('cat')\n",
    "#alice_text_obj.common_contexts(['cat'])\n",
    "\n",
    "%matplotlib inline\n",
    "alice_text_obj.dispersion_plot(['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 3\n",
    "\n",
    "###Stemming\n",
    "What:  Reduce a word to its base/stem form\n",
    "\n",
    "Why:   Often makes sense to treat multiple word forms the same way\n",
    "\n",
    "Notes: Uses a \"simple\" and fast rule-based approach\n",
    "       Output can be undesirable for irregular words\n",
    "       Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "       Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'run'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an English stemmer that uses the Snowball technique\n",
    "# nltk.stem.snowball.SnowballStemmer\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "Stemmer = SnowballStemmer('english')\n",
    "Stemmer.stem('running')\n",
    "\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "#Stemmer = PorterStemmer()\n",
    "#Stemmer.stem('running')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stemmer = PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'charg'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the following words: charge, charging, charged\n",
    "Stemmer.stem('charge')\n",
    "Stemmer.stem('charging')\n",
    "Stemmer.stem('charged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'chrged'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can you stem \"words\" with punctuation in them? Or which have no letters?\n",
    "Stemmer.stem('charged.')\n",
    "Stemmer.stem('chrged')\n",
    "\n",
    "#No you cannot stem \"words\" with punctuation or no letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new list of words from the novels by dropping out spurious non-words.\n",
    "# You might find word_is_just_letters() helpful\n",
    "def word_is_just_letters(word):\n",
    "    import re\n",
    "    return re.search('^[a-zA-Z]+', word)\n",
    "\n",
    "\n",
    "def is_a_real_word(x):\n",
    "    return x not in \".,[]:!'?&-;()\"\n",
    "\n",
    "cleaned_up_alice = [\n",
    "    x.lower() \n",
    "        for x in alice_distinct_words\n",
    "        if word_is_just_letters(x)\n",
    "]\n",
    "cleaned_up_emma = [\n",
    "    x.lower() \n",
    "        for x in emma_distinct_words\n",
    "        if word_is_just_letters(x)\n",
    "]\n",
    "\n",
    "#cleaned_up_alice\n",
    "#cleaned_up_emma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'yellow',\n",
       " u'four',\n",
       " u'doe',\n",
       " u'hang',\n",
       " u'aggress',\n",
       " u'brisk',\n",
       " u'look',\n",
       " u'elig',\n",
       " u'snow',\n",
       " u'rous',\n",
       " u'scold',\n",
       " u'unansw',\n",
       " u'fetch',\n",
       " u'lord',\n",
       " u'elig',\n",
       " u'meadow',\n",
       " u'sink',\n",
       " u'stipul',\n",
       " u'sweetbread',\n",
       " u'stab',\n",
       " u'bring',\n",
       " u'disturb',\n",
       " u'recollect',\n",
       " u'scholar',\n",
       " u'wood',\n",
       " u'acquit',\n",
       " u'tire',\n",
       " u'cordial',\n",
       " u'prefac',\n",
       " u'puls',\n",
       " u'tire',\n",
       " u'eleg',\n",
       " u'second',\n",
       " u'consent',\n",
       " u'error',\n",
       " u'contribut',\n",
       " u'wrong',\n",
       " u'increas',\n",
       " u'induc',\n",
       " u'hero',\n",
       " u'intellect',\n",
       " u'interrupt',\n",
       " u'here',\n",
       " u'properest',\n",
       " u'her',\n",
       " u'natur',\n",
       " u'substanc',\n",
       " u'garden',\n",
       " u'unwilling',\n",
       " u'pretens',\n",
       " u'report',\n",
       " u'militari',\n",
       " u'critic',\n",
       " u'divid',\n",
       " u'seclud',\n",
       " u'explain',\n",
       " u'civil',\n",
       " u'summon',\n",
       " u'brought',\n",
       " u'cherish',\n",
       " u'spoke',\n",
       " u'reproach',\n",
       " u'imparti',\n",
       " u'music',\n",
       " u'therefor',\n",
       " u'strike',\n",
       " u'femal',\n",
       " u'success',\n",
       " u'bring',\n",
       " u'whirl',\n",
       " u'hurt',\n",
       " u'glass',\n",
       " u'midst',\n",
       " u'hold',\n",
       " u'circumst',\n",
       " u'lock',\n",
       " u'pursu',\n",
       " u'accomplish',\n",
       " u'eatabl',\n",
       " u'exampl',\n",
       " u'misfortun',\n",
       " u'vaniti',\n",
       " u'la',\n",
       " u'unjust',\n",
       " u'caution',\n",
       " u'want',\n",
       " u'reprov',\n",
       " u'absolut',\n",
       " u'travel',\n",
       " u'featur',\n",
       " u'how',\n",
       " u'hot',\n",
       " u'hesit',\n",
       " u'hop',\n",
       " u'prefer',\n",
       " u'dignifi',\n",
       " u'a',\n",
       " u'bate',\n",
       " u'beauti',\n",
       " u'charm',\n",
       " u'thank',\n",
       " u'wrong',\n",
       " u'destin',\n",
       " u'charad',\n",
       " u'unselfish',\n",
       " u'testifi',\n",
       " u'ballroom',\n",
       " u'endeavour',\n",
       " u'presid',\n",
       " u'wink',\n",
       " u'anoth',\n",
       " u'wind',\n",
       " u'wine',\n",
       " u'blind',\n",
       " u'welcom',\n",
       " u'concurr',\n",
       " u'enquir',\n",
       " u'donwel',\n",
       " u'reward',\n",
       " u'dreamer',\n",
       " u'admir',\n",
       " u'his',\n",
       " u'abbot',\n",
       " u'fit',\n",
       " u'heretofor',\n",
       " u'fix',\n",
       " u'occup',\n",
       " u'smith',\n",
       " u'distinguish',\n",
       " u'admir',\n",
       " u'easier',\n",
       " u'grievous',\n",
       " u'enrich',\n",
       " u'educ',\n",
       " u'effect',\n",
       " u'sixteen',\n",
       " u'silver',\n",
       " u'prize',\n",
       " u'rumour',\n",
       " u'blush',\n",
       " u'arrow',\n",
       " u'blush',\n",
       " u'preced',\n",
       " u'whim',\n",
       " u'blinder',\n",
       " u'turnip',\n",
       " u'preced',\n",
       " u'fortnight',\n",
       " u'displeas',\n",
       " u'allay',\n",
       " u'messag',\n",
       " u'backgammon',\n",
       " u'born',\n",
       " u'misfortun',\n",
       " u'forward',\n",
       " u'indiscret',\n",
       " u'except',\n",
       " u're',\n",
       " u'encourag',\n",
       " u'adapt',\n",
       " u'mend',\n",
       " u'vaniti',\n",
       " u'foundat',\n",
       " u'assur',\n",
       " u'adopt',\n",
       " u'threaten',\n",
       " u'check',\n",
       " u'estim',\n",
       " u'contrit',\n",
       " u'soft',\n",
       " u'univers',\n",
       " u'enorm',\n",
       " u'ate',\n",
       " u'moment',\n",
       " u'disturb',\n",
       " u'speedi',\n",
       " u'human',\n",
       " u'loud',\n",
       " u'play',\n",
       " u'clamor',\n",
       " u'elm',\n",
       " u'servic',\n",
       " u'engag',\n",
       " u'need',\n",
       " u'master',\n",
       " u'bitter',\n",
       " u'listen',\n",
       " u'how',\n",
       " u'oddest',\n",
       " u'posit',\n",
       " u'beauti',\n",
       " u'exult',\n",
       " u'tree',\n",
       " u'shower',\n",
       " u'idl',\n",
       " u'exclaim',\n",
       " u'endur',\n",
       " u'seminari',\n",
       " u'feel',\n",
       " u'festiv',\n",
       " u'explor',\n",
       " u'humour',\n",
       " u'shrub',\n",
       " u'thaw',\n",
       " u'dozen',\n",
       " u'affair',\n",
       " u'forgiv',\n",
       " u'wholesom',\n",
       " u'reveri',\n",
       " u'eager',\n",
       " u'recommend',\n",
       " u'amus',\n",
       " u'they',\n",
       " u'imprud',\n",
       " u'shall',\n",
       " u'wish',\n",
       " u'object',\n",
       " u'mouth',\n",
       " u'letter',\n",
       " u'retain',\n",
       " u'moral',\n",
       " u'kingston',\n",
       " u'scream',\n",
       " u'came',\n",
       " u'say',\n",
       " u'composur',\n",
       " u'meet',\n",
       " u'undu',\n",
       " u'falsehood',\n",
       " u'tempt',\n",
       " u'lesson',\n",
       " u'touch',\n",
       " u'busi',\n",
       " u'vexat',\n",
       " u'light',\n",
       " u'quaint',\n",
       " u'cousin',\n",
       " u'touch',\n",
       " u'rich',\n",
       " u'heartili',\n",
       " u'rice',\n",
       " u'emphat',\n",
       " u'churchil',\n",
       " u'terror',\n",
       " u'pocket',\n",
       " u'altogeth',\n",
       " u'x',\n",
       " u'relish',\n",
       " u'nice',\n",
       " u'boarder',\n",
       " u'patch',\n",
       " u'esteem',\n",
       " u'adher',\n",
       " u'hasten',\n",
       " u'fair',\n",
       " u'expedi',\n",
       " u'unexpect',\n",
       " u'result',\n",
       " u'fail',\n",
       " u'john',\n",
       " u'best',\n",
       " u'avert',\n",
       " u'play',\n",
       " u'solicitud',\n",
       " u'loth',\n",
       " u'preserv',\n",
       " u'let',\n",
       " u'men',\n",
       " u'extend',\n",
       " u'natur',\n",
       " u'councillor',\n",
       " u'twinkl',\n",
       " u'extent',\n",
       " u'defianc',\n",
       " u'dexter',\n",
       " u'debt',\n",
       " u'piti',\n",
       " u'accid',\n",
       " u'sacrif',\n",
       " u'refin',\n",
       " u'countri',\n",
       " u'bless',\n",
       " u'demand',\n",
       " u'plan',\n",
       " u'marri',\n",
       " u'ask',\n",
       " u'pre',\n",
       " u'mortifi',\n",
       " u'honour',\n",
       " u'vain',\n",
       " u'liabl',\n",
       " u'weari',\n",
       " u'parley',\n",
       " u'union',\n",
       " u'aspers',\n",
       " u'subsid',\n",
       " u'much',\n",
       " u'privileg',\n",
       " u'life',\n",
       " u'retrospect',\n",
       " u'point',\n",
       " u'confin',\n",
       " u'child',\n",
       " u'evas',\n",
       " u'work',\n",
       " u'doth',\n",
       " u'divert',\n",
       " u'contempl',\n",
       " u'conting',\n",
       " u'employ',\n",
       " u'misconstru',\n",
       " u'unfelt',\n",
       " u'campbel',\n",
       " u'goddard',\n",
       " u'rememb',\n",
       " u'congratul',\n",
       " u'play',\n",
       " u'player',\n",
       " u'eighteen',\n",
       " u'anticip',\n",
       " u'trust',\n",
       " u'memori',\n",
       " u'sever',\n",
       " u'thing',\n",
       " u'woollen',\n",
       " u'ani',\n",
       " u'haphazard',\n",
       " u'harmoni',\n",
       " u'princip',\n",
       " u'exert',\n",
       " u'fair',\n",
       " u'boil',\n",
       " u'consent',\n",
       " u'contrari',\n",
       " u'supper',\n",
       " u'tune',\n",
       " u'small',\n",
       " u'cold',\n",
       " u'veil',\n",
       " u'opinion',\n",
       " u'outliv',\n",
       " u'thank',\n",
       " u'absurd',\n",
       " u'disguis',\n",
       " u'exclaim',\n",
       " u'succeed',\n",
       " u'previous',\n",
       " u'spectacl',\n",
       " u'dialogu',\n",
       " u'eas',\n",
       " u'had',\n",
       " u'hay',\n",
       " u'dispar',\n",
       " u'collect',\n",
       " u'belov',\n",
       " u'has',\n",
       " u'hat',\n",
       " u'elev',\n",
       " u'disagr',\n",
       " u'possibl',\n",
       " u'three',\n",
       " u'unequivoc',\n",
       " u'possibl',\n",
       " u'chuse',\n",
       " u'birth',\n",
       " u'shadow',\n",
       " u'desir',\n",
       " u'lord',\n",
       " u'seasid',\n",
       " u'remind',\n",
       " u'misl',\n",
       " u'stept',\n",
       " u'step',\n",
       " u'prologu',\n",
       " u'effus',\n",
       " u'attorney',\n",
       " u'right',\n",
       " u'old',\n",
       " u'crowd',\n",
       " u'peopl',\n",
       " u'crown',\n",
       " u'sure',\n",
       " u'between',\n",
       " u'creep',\n",
       " u'enemi',\n",
       " u'for',\n",
       " u'communic',\n",
       " u'contribut',\n",
       " u'summon',\n",
       " u'ponder',\n",
       " u'palat',\n",
       " u'extenu',\n",
       " u'lose',\n",
       " u'moralis',\n",
       " u'think',\n",
       " u'shaken',\n",
       " u'suckl',\n",
       " u'restless',\n",
       " u'visitor',\n",
       " u'grievous',\n",
       " u'recollect',\n",
       " u'despair',\n",
       " u'repel',\n",
       " u'slight',\n",
       " u'consult',\n",
       " u'nought',\n",
       " u'scotland',\n",
       " u'respect',\n",
       " u'son',\n",
       " u'administ',\n",
       " u'respect',\n",
       " u'be',\n",
       " u'wrap',\n",
       " u'alderney',\n",
       " u'wait',\n",
       " u'support',\n",
       " u'constant',\n",
       " u'avail',\n",
       " u'great',\n",
       " u'result',\n",
       " u'happi',\n",
       " u'offer',\n",
       " u'form',\n",
       " u'talent',\n",
       " u'congratul',\n",
       " u'sir',\n",
       " u'later',\n",
       " u'disgrac',\n",
       " u'six',\n",
       " u'sooth',\n",
       " u'prove',\n",
       " u'exist',\n",
       " u'account',\n",
       " u'effus',\n",
       " u'protest',\n",
       " u'solicit',\n",
       " u'floor',\n",
       " u'utter',\n",
       " u'grammat',\n",
       " u'confer',\n",
       " u'irresist',\n",
       " u'smell',\n",
       " u'e',\n",
       " u'transport',\n",
       " u'deathb',\n",
       " u'intent',\n",
       " u'mad',\n",
       " u'madeira',\n",
       " u'time',\n",
       " u'push',\n",
       " u'confer',\n",
       " u'gown',\n",
       " u'whoever',\n",
       " u'chair',\n",
       " u'ware',\n",
       " u'rous',\n",
       " u'indol',\n",
       " u'vex',\n",
       " u'recurr',\n",
       " u'believ',\n",
       " u'fatigu',\n",
       " u'choic',\n",
       " u'gloomi',\n",
       " u'raptur',\n",
       " u'ration',\n",
       " u'stay',\n",
       " u'fullest',\n",
       " u'exact',\n",
       " u'minut',\n",
       " u'tear',\n",
       " u'leav',\n",
       " u'settl',\n",
       " u'tan',\n",
       " u'prevent',\n",
       " u'occurr',\n",
       " u'sigh',\n",
       " u'steak',\n",
       " u'lengthen',\n",
       " u'educ',\n",
       " u'proprieti',\n",
       " u'unfeel',\n",
       " u'freak',\n",
       " u'deaden',\n",
       " u'fall',\n",
       " u'ground',\n",
       " u'true',\n",
       " u'me',\n",
       " u'ma',\n",
       " u'honour',\n",
       " u'funer',\n",
       " u'understand',\n",
       " u'contempl',\n",
       " u'yard',\n",
       " u'mr',\n",
       " u'alon',\n",
       " u'like',\n",
       " u'my',\n",
       " u'brilliant',\n",
       " u'coxcomb',\n",
       " u'studi',\n",
       " u'wherev',\n",
       " u'common',\n",
       " u'accomplish',\n",
       " u'my',\n",
       " u'conceal',\n",
       " u'love',\n",
       " u'prefer',\n",
       " u'abbey',\n",
       " u'betray',\n",
       " u'xviii',\n",
       " u'august',\n",
       " u'work',\n",
       " u'posit',\n",
       " u'angri',\n",
       " u'predict',\n",
       " u'papa',\n",
       " u'imagin',\n",
       " u'vigour',\n",
       " u'oppos',\n",
       " u'wonder',\n",
       " u'those',\n",
       " u'wick',\n",
       " u'introduc',\n",
       " u'concili',\n",
       " u'afford',\n",
       " u'subsist',\n",
       " u'appar',\n",
       " u'refrain',\n",
       " u'approv',\n",
       " u'everywher',\n",
       " u'virtu',\n",
       " u'behalf',\n",
       " u'valu',\n",
       " u'anyth',\n",
       " u'origin',\n",
       " u'pretend',\n",
       " u'midsumm',\n",
       " u'privi',\n",
       " u'inflam',\n",
       " u'believ',\n",
       " u'valu',\n",
       " u'believ',\n",
       " u'our',\n",
       " u'detach',\n",
       " u'admir',\n",
       " u'parish',\n",
       " u'greatcoat',\n",
       " u'unequal',\n",
       " u'admir',\n",
       " u'allow',\n",
       " u'evid',\n",
       " u'winter',\n",
       " u'divid',\n",
       " u'who',\n",
       " u'vain',\n",
       " u'thank',\n",
       " u'whi',\n",
       " u'undertook',\n",
       " u'absent',\n",
       " u'spot',\n",
       " u'applic',\n",
       " u'explor',\n",
       " u'date',\n",
       " u'such',\n",
       " u'reveal',\n",
       " u'disagre',\n",
       " u'captain',\n",
       " u'natur',\n",
       " u'varieti',\n",
       " u'conscious',\n",
       " u'ordinarili',\n",
       " u'attest',\n",
       " u'so',\n",
       " u'snow',\n",
       " u'se',\n",
       " u'troubl',\n",
       " u'privat',\n",
       " u'barn',\n",
       " u'footstep',\n",
       " u'year',\n",
       " u'cours',\n",
       " u'quarrel',\n",
       " u'omit',\n",
       " u'pollard',\n",
       " u'tendenc',\n",
       " u'solitari',\n",
       " u'solac',\n",
       " u'troubl',\n",
       " u'stupid',\n",
       " u'attract',\n",
       " u'suspicion',\n",
       " u'suspens',\n",
       " u'instant',\n",
       " u'convey',\n",
       " u'match',\n",
       " u'remonstr',\n",
       " u'overpow',\n",
       " u'arriv',\n",
       " u'epithet',\n",
       " u'reckon',\n",
       " u'twilight',\n",
       " u'maintain',\n",
       " u'match',\n",
       " u'quarter',\n",
       " u'cordial',\n",
       " u'squar',\n",
       " u'receipt',\n",
       " u'm',\n",
       " u'owe',\n",
       " u'enter',\n",
       " u'pale',\n",
       " u'neighbourhood',\n",
       " u'elizabeth',\n",
       " u'abid',\n",
       " u'serious',\n",
       " u'suggest',\n",
       " u'border',\n",
       " u'oftentim',\n",
       " u'plot',\n",
       " u'compliment',\n",
       " u'hairdress',\n",
       " u'million',\n",
       " u'possibl',\n",
       " u'quit',\n",
       " u'disrespect',\n",
       " u'offend',\n",
       " u'either',\n",
       " u'besid',\n",
       " u'quit',\n",
       " u'remaind',\n",
       " u'liber',\n",
       " u'patronag',\n",
       " u'modest',\n",
       " u'cross',\n",
       " u'undesir',\n",
       " u'neglect',\n",
       " u'emot',\n",
       " u'save',\n",
       " u'proper',\n",
       " u'spoken',\n",
       " u'one',\n",
       " u'habit',\n",
       " u'open',\n",
       " u'awkward',\n",
       " u'afford',\n",
       " u'languish',\n",
       " u'goodhumour',\n",
       " u'solitarili',\n",
       " u'vouchsaf',\n",
       " u'indic',\n",
       " u'dissip',\n",
       " u'encourag',\n",
       " u'linger',\n",
       " u'press',\n",
       " u'shawl',\n",
       " u'prove',\n",
       " u'ridicul',\n",
       " u'waiv',\n",
       " u'chilblain',\n",
       " u'sept',\n",
       " u'seven',\n",
       " u'companion',\n",
       " u'overtaken',\n",
       " u'depress',\n",
       " u'rival',\n",
       " u'folli',\n",
       " u'futur',\n",
       " u'win',\n",
       " u'wander',\n",
       " u'prospect',\n",
       " u'address',\n",
       " u'ill',\n",
       " u'nicer',\n",
       " u'turn',\n",
       " u'argument',\n",
       " u'jewel',\n",
       " u'sad',\n",
       " u'say',\n",
       " u'rain',\n",
       " u'buri',\n",
       " u'lurk',\n",
       " u'uninterrupt',\n",
       " u'saw',\n",
       " u'sat',\n",
       " u'fashion',\n",
       " u'unsuit',\n",
       " u'asid',\n",
       " u'note',\n",
       " u'rumin',\n",
       " u'transcrib',\n",
       " u'take',\n",
       " u'half',\n",
       " u'want',\n",
       " u'hall',\n",
       " u'credit',\n",
       " u'appel',\n",
       " u'alter',\n",
       " u'opposit',\n",
       " u'syllabl',\n",
       " u'discern',\n",
       " u'knew',\n",
       " u'compress',\n",
       " u'remark',\n",
       " u'knee',\n",
       " u'page',\n",
       " u'lawn',\n",
       " u'guidabl',\n",
       " u'blockhead',\n",
       " u'drive',\n",
       " u'weston',\n",
       " u'way',\n",
       " u'salt',\n",
       " u'trembl',\n",
       " u'walk',\n",
       " u'murmur',\n",
       " u'pencil',\n",
       " u'imagin',\n",
       " u'merit',\n",
       " u'bright',\n",
       " u'inconsist',\n",
       " u'scarc',\n",
       " u'imagin',\n",
       " u'slow',\n",
       " u'unkind',\n",
       " u'cloak',\n",
       " u'tear',\n",
       " u'go',\n",
       " u'equip',\n",
       " u'uppermost',\n",
       " u'entreati',\n",
       " u'wil',\n",
       " u'guard',\n",
       " u'rejoic',\n",
       " u'assist',\n",
       " u'xiv',\n",
       " u'xix',\n",
       " u'clearest',\n",
       " u'prime',\n",
       " u'keen',\n",
       " u'artist',\n",
       " u'borrow',\n",
       " u'manchest',\n",
       " u'though',\n",
       " u'liabl',\n",
       " u'where',\n",
       " u'vision',\n",
       " u'gout',\n",
       " u'purest',\n",
       " u'mutton',\n",
       " u'that',\n",
       " u'alarm',\n",
       " u'ostens',\n",
       " u'affect',\n",
       " u'illiber',\n",
       " u'untaint',\n",
       " u'screen',\n",
       " u'grate',\n",
       " u'reign',\n",
       " u'aggrandis',\n",
       " u'suppress',\n",
       " u'concentr',\n",
       " u'convers',\n",
       " u'braithwait',\n",
       " u'cramp',\n",
       " u'resid',\n",
       " u'deduct',\n",
       " u's',\n",
       " u'loud',\n",
       " u'express',\n",
       " u'doctor',\n",
       " u'allow',\n",
       " u'riddl',\n",
       " u'inclin',\n",
       " u'hurt',\n",
       " u'gaieti',\n",
       " u'weston',\n",
       " u'consid',\n",
       " u'care',\n",
       " u'plantat',\n",
       " u'stretch',\n",
       " u'vacat',\n",
       " u'breath',\n",
       " u'practis',\n",
       " u'combin',\n",
       " u'motiv',\n",
       " u'want',\n",
       " u'enabl',\n",
       " u'whenev',\n",
       " u'influenc',\n",
       " u'thousand',\n",
       " u'form',\n",
       " u'observ',\n",
       " u'former',\n",
       " u'fell',\n",
       " u'pretenc',\n",
       " u'singl',\n",
       " u'him',\n",
       " u'intim',\n",
       " u'convey',\n",
       " u'newspap',\n",
       " u'situat',\n",
       " u'endur',\n",
       " u'preval',\n",
       " u'engag',\n",
       " u'dubious',\n",
       " u'regular',\n",
       " u'presid',\n",
       " u'engag',\n",
       " u'fame',\n",
       " u'toward',\n",
       " u'verifi',\n",
       " u'preparatori',\n",
       " u'perpetu',\n",
       " u'forebod',\n",
       " u'respit',\n",
       " u'irksom',\n",
       " u'sick',\n",
       " u'i',\n",
       " u'obtrud',\n",
       " u'xvi',\n",
       " u'anywher',\n",
       " u'unmanag',\n",
       " u'flock',\n",
       " u'vacant',\n",
       " u'bailiff',\n",
       " u'commonest',\n",
       " u'peculiar',\n",
       " u'convers',\n",
       " u'summer',\n",
       " u'be',\n",
       " u'rest',\n",
       " u'dispers',\n",
       " u'afterward',\n",
       " u'presumpt',\n",
       " u'cogit',\n",
       " u'medal',\n",
       " u'hereabout',\n",
       " u'instrument',\n",
       " u'dryli',\n",
       " u'overthrow',\n",
       " u'joy',\n",
       " u'around',\n",
       " u'sum',\n",
       " u'dark',\n",
       " u'traffic',\n",
       " u'prefer',\n",
       " u'world',\n",
       " u'apprehend',\n",
       " u'dare',\n",
       " u'meat',\n",
       " u'stranger',\n",
       " u'superior',\n",
       " u'prophet',\n",
       " u'stationari',\n",
       " u'completest',\n",
       " u'perri',\n",
       " u'woodhous',\n",
       " u'moreov',\n",
       " u'think',\n",
       " u'dimens',\n",
       " u'restor',\n",
       " u'refer',\n",
       " u'unsuccess',\n",
       " u'power',\n",
       " u'intim',\n",
       " u'luxuri',\n",
       " u'stone',\n",
       " u'joy',\n",
       " u'industri',\n",
       " u'side',\n",
       " u'practic',\n",
       " u'act',\n",
       " u'luck',\n",
       " u'sleeti',\n",
       " u'no',\n",
       " u'antidot',\n",
       " u'sneer',\n",
       " u'widow',\n",
       " u'live',\n",
       " u'parti',\n",
       " u'her',\n",
       " u'endeavour',\n",
       " u'meal',\n",
       " u'seal',\n",
       " u'seri',\n",
       " u'complet',\n",
       " u'danger',\n",
       " u'fidgeti',\n",
       " u'land',\n",
       " u'harri',\n",
       " u'small',\n",
       " u'with',\n",
       " u'handsom',\n",
       " u'abus',\n",
       " u'rush',\n",
       " u'nearest',\n",
       " u'imput',\n",
       " u'dirti',\n",
       " u'dispens',\n",
       " u'agre',\n",
       " u'touchston',\n",
       " u'darker',\n",
       " u'detail',\n",
       " u'gone',\n",
       " u'fright',\n",
       " u'exhaust',\n",
       " u'ah',\n",
       " u'accent',\n",
       " u'am',\n",
       " u'al',\n",
       " u'watch',\n",
       " u'an',\n",
       " u'how',\n",
       " u'as',\n",
       " u'at',\n",
       " u'walk',\n",
       " u'watch',\n",
       " u'tranquil',\n",
       " u'admir',\n",
       " u'trembl',\n",
       " u'cream',\n",
       " u'real',\n",
       " u'beam',\n",
       " u'read',\n",
       " u'sanguin',\n",
       " u'blind',\n",
       " u'quarrel',\n",
       " u'incomprehens',\n",
       " u'ladi',\n",
       " u'contend',\n",
       " u'regardless',\n",
       " u'accost',\n",
       " u'serious',\n",
       " u'gayest',\n",
       " u'account',\n",
       " u'conjug',\n",
       " u'madam',\n",
       " u'vicar',\n",
       " u'trick',\n",
       " u'dy',\n",
       " u'imposs',\n",
       " u'sensibl',\n",
       " u'origin',\n",
       " u'waiv',\n",
       " u'influenza',\n",
       " u'consid',\n",
       " u'welfar',\n",
       " u'caus',\n",
       " u'improp',\n",
       " u'care',\n",
       " u'hunt',\n",
       " u'to',\n",
       " u'mickleham',\n",
       " u'dress',\n",
       " u'smile',\n",
       " u'case',\n",
       " u'return',\n",
       " u'puzzl',\n",
       " u'float',\n",
       " u'puzzl',\n",
       " u'condit',\n",
       " u'accompani',\n",
       " u'ingratitud',\n",
       " u'confeder',\n",
       " u'lay',\n",
       " u'join',\n",
       " u'larg',\n",
       " u'sang',\n",
       " u'small',\n",
       " u'quicker',\n",
       " u'method',\n",
       " u'honour',\n",
       " u'should',\n",
       " u'past',\n",
       " u'carriag',\n",
       " u'display',\n",
       " u'pass',\n",
       " u'warmth',\n",
       " u'quicken',\n",
       " u'clock',\n",
       " u'forestal',\n",
       " u'prevail',\n",
       " u'undecid',\n",
       " u'nurs',\n",
       " u'seem',\n",
       " u'method',\n",
       " u'contrast',\n",
       " u'full',\n",
       " u'escap',\n",
       " u'indecis',\n",
       " u'hour',\n",
       " u'conclud',\n",
       " u'complianc',\n",
       " u'experi',\n",
       " u'acquitt',\n",
       " u'pick',\n",
       " u'action',\n",
       " u'outstep',\n",
       " u'sweet',\n",
       " u'follow',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem all those words\n",
    "\n",
    "alice_stems = [Stemmer.stem(word) for word in cleaned_up_alice]\n",
    "alice_stems\n",
    "emma_stems = [Stemmer.stem(word) for word in cleaned_up_emma]\n",
    "emma_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'respect', 11),\n",
       " (u'observ', 10),\n",
       " (u'thank', 8),\n",
       " (u'play', 8),\n",
       " (u'admir', 8),\n",
       " (u'like', 8),\n",
       " (u'relat', 7),\n",
       " (u'affect', 7),\n",
       " (u'delight', 7),\n",
       " (u'depend', 7),\n",
       " (u'approv', 7),\n",
       " (u'care', 7),\n",
       " (u'open', 7),\n",
       " (u'desir', 7),\n",
       " (u'suffer', 7),\n",
       " (u'fanci', 7),\n",
       " (u'imagin', 7),\n",
       " (u'invit', 7),\n",
       " (u'live', 7),\n",
       " (u'engag', 7),\n",
       " (u'encourag', 7),\n",
       " (u'enjoy', 6),\n",
       " (u'object', 6),\n",
       " (u'accord', 6),\n",
       " (u'assur', 6)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create two collections.Counter objects (one for each novel)\n",
    "# so that you can easily count word stems. If you give\n",
    "# the stemmed lists as an argument to constructor, \n",
    "# you can use .most_common(25) to get the top 25 tokens\n",
    "\n",
    "from collections import Counter\n",
    "alice_cnt = Counter() \n",
    "for word in alice_stems:\n",
    "    alice_cnt[word] += 1\n",
    "alice_cnt.most_common(25)\n",
    "\n",
    "emma_cnt = Counter() \n",
    "for word in emma_stems:\n",
    "    emma_cnt[word] += 1\n",
    "emma_cnt.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lemmatization / synset\n",
    "What:  Derive the canonical form ('lemma') of a word\n",
    "    \n",
    "Why:   Can be better than stemming, reduces words to a 'normal' form.\n",
    "    \n",
    "Notes: Uses a dictionary-based approach (slower than stemming)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What synsets does 'dog' belong to?\n",
    "nltk.corpus.wordnet.synsets('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which synset is the one you were thinking of?\n",
    "nltk.corpus.wordnet.synsets('dog')[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('canine.n.02')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is its hypernym?\n",
    "dog = nltk.corpus.wordnet.synsets('dog')[0]\n",
    "dog.hypernyms()[0]\n",
    "#dog.hypernyms()[0].hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('canine.n.02')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What about wolves? What synsets does it belong to?\n",
    "wolves = nltk.corpus.wordnet.synsets('wolves')[0]\n",
    "wolves.hypernyms()[0]\n",
    "#wolves.hypernyms()[0].hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How closely related are those concepts (dogs and wolves)?\n",
    "wolves.path_similarity(dog)\n",
    "#wolves.lowest_common_hypernyms(dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How closely related are the concepts 'dog' and 'novel'?\n",
    "dog.lowest_common_hypernyms(nltk.corpus.wordnet.synsets('novel')[0])\n",
    "dog.path_similarity(nltk.corpus.wordnet.synsets('novel')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 3 Part of speech tagging\n",
    "\n",
    "Other:\n",
    "- Analysing data with the Alchemy API\n",
    "- Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Part of Speech Tagging\n",
    "\n",
    "What:  Determine the part of speech of a word\n",
    "    \n",
    "Why:   This can inform other methods and models such as Named Entity Recognition\n",
    "    \n",
    "Notes: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('learning', 'VBG'),\n",
       " ('Data', 'NNP'),\n",
       " ('Science', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('soon', 'RB'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('working', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('this', 'DT')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use nltk.pos_tag to parse a sentence\n",
    "nltk.pos_tag(nltk.word_tokenize('I am learning Data Science and soon would be working on this'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (Optional for the enthusiastic)\n",
    "# What verbs did Jane Austen use a lot of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 4\n",
    "###Stopword Removal\n",
    "\n",
    "What:  Remove common words that will likely appear in any text\n",
    "    \n",
    "Why:   They don't tell you much about your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-c06df35ae903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# most of top 25 stemmed tokens are \"worthless\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "# most of top 25 stemmed tokens are \"worthless\"\n",
    "c.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'a',\n",
       " u'about',\n",
       " u'above',\n",
       " u'after',\n",
       " u'again',\n",
       " u'against',\n",
       " u'ain',\n",
       " u'all',\n",
       " u'am',\n",
       " u'an',\n",
       " u'and',\n",
       " u'any',\n",
       " u'are',\n",
       " u'aren',\n",
       " u'as',\n",
       " u'at',\n",
       " u'be',\n",
       " u'because',\n",
       " u'been',\n",
       " u'before',\n",
       " u'being',\n",
       " u'below',\n",
       " u'between',\n",
       " u'both',\n",
       " u'but',\n",
       " u'by',\n",
       " u'can',\n",
       " u'couldn',\n",
       " u'd',\n",
       " u'did',\n",
       " u'didn',\n",
       " u'do',\n",
       " u'does',\n",
       " u'doesn',\n",
       " u'doing',\n",
       " u'don',\n",
       " u'down',\n",
       " u'during',\n",
       " u'each',\n",
       " u'few',\n",
       " u'for',\n",
       " u'from',\n",
       " u'further',\n",
       " u'had',\n",
       " u'hadn',\n",
       " u'has',\n",
       " u'hasn',\n",
       " u'have',\n",
       " u'haven',\n",
       " u'having',\n",
       " u'he',\n",
       " u'her',\n",
       " u'here',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'him',\n",
       " u'himself',\n",
       " u'his',\n",
       " u'how',\n",
       " u'i',\n",
       " u'if',\n",
       " u'in',\n",
       " u'into',\n",
       " u'is',\n",
       " u'isn',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'just',\n",
       " u'll',\n",
       " u'm',\n",
       " u'ma',\n",
       " u'me',\n",
       " u'mightn',\n",
       " u'more',\n",
       " u'most',\n",
       " u'mustn',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'needn',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'now',\n",
       " u'o',\n",
       " u'of',\n",
       " u'off',\n",
       " u'on',\n",
       " u'once',\n",
       " u'only',\n",
       " u'or',\n",
       " u'other',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'out',\n",
       " u'over',\n",
       " u'own',\n",
       " u're',\n",
       " u's',\n",
       " u'same',\n",
       " u'shan',\n",
       " u'she',\n",
       " u'should',\n",
       " u'shouldn',\n",
       " u'so',\n",
       " u'some',\n",
       " u'such',\n",
       " u't',\n",
       " u'than',\n",
       " u'that',\n",
       " u'the',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'them',\n",
       " u'themselves',\n",
       " u'then',\n",
       " u'there',\n",
       " u'these',\n",
       " u'they',\n",
       " u'this',\n",
       " u'those',\n",
       " u'through',\n",
       " u'to',\n",
       " u'too',\n",
       " u'under',\n",
       " u'until',\n",
       " u'up',\n",
       " u've',\n",
       " u'very',\n",
       " u'was',\n",
       " u'wasn',\n",
       " u'we',\n",
       " u'were',\n",
       " u'weren',\n",
       " u'what',\n",
       " u'when',\n",
       " u'where',\n",
       " u'which',\n",
       " u'while',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'why',\n",
       " u'will',\n",
       " u'with',\n",
       " u'won',\n",
       " u'wouldn',\n",
       " u'y',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the list of stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "sorted(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'our',\n",
       " u'ourselv',\n",
       " u'you',\n",
       " u'your',\n",
       " u'your',\n",
       " u'yourself',\n",
       " u'yourselv',\n",
       " u'he',\n",
       " u'him',\n",
       " u'his',\n",
       " u'himself',\n",
       " u'she',\n",
       " u'her',\n",
       " u'her',\n",
       " u'herself',\n",
       " u'it',\n",
       " u'it',\n",
       " u'itself',\n",
       " u'they',\n",
       " u'them',\n",
       " u'their',\n",
       " u'their',\n",
       " u'themselv',\n",
       " u'what',\n",
       " u'which',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'this',\n",
       " u'that',\n",
       " u'these',\n",
       " u'those',\n",
       " u'am',\n",
       " u'is',\n",
       " u'are',\n",
       " u'was',\n",
       " u'were',\n",
       " u'be',\n",
       " u'been',\n",
       " u'be',\n",
       " u'have',\n",
       " u'has',\n",
       " u'had',\n",
       " u'have',\n",
       " u'do',\n",
       " u'doe',\n",
       " u'did',\n",
       " u'do',\n",
       " u'a',\n",
       " u'an',\n",
       " u'the',\n",
       " u'and',\n",
       " u'but',\n",
       " u'if',\n",
       " u'or',\n",
       " u'becaus',\n",
       " u'as',\n",
       " u'until',\n",
       " u'while',\n",
       " u'of',\n",
       " u'at',\n",
       " u'by',\n",
       " u'for',\n",
       " u'with',\n",
       " u'about',\n",
       " u'against',\n",
       " u'between',\n",
       " u'into',\n",
       " u'through',\n",
       " u'dure',\n",
       " u'befor',\n",
       " u'after',\n",
       " u'abov',\n",
       " u'below',\n",
       " u'to',\n",
       " u'from',\n",
       " u'up',\n",
       " u'down',\n",
       " u'in',\n",
       " u'out',\n",
       " u'on',\n",
       " u'off',\n",
       " u'over',\n",
       " u'under',\n",
       " u'again',\n",
       " u'further',\n",
       " u'then',\n",
       " u'onc',\n",
       " u'here',\n",
       " u'there',\n",
       " u'when',\n",
       " u'where',\n",
       " u'whi',\n",
       " u'how',\n",
       " u'all',\n",
       " u'ani',\n",
       " u'both',\n",
       " u'each',\n",
       " u'few',\n",
       " u'more',\n",
       " u'most',\n",
       " u'other',\n",
       " u'some',\n",
       " u'such',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'onli',\n",
       " u'own',\n",
       " u'same',\n",
       " u'so',\n",
       " u'than',\n",
       " u'too',\n",
       " u'veri',\n",
       " u's',\n",
       " u't',\n",
       " u'can',\n",
       " u'will',\n",
       " u'just',\n",
       " u'don',\n",
       " u'should',\n",
       " u'now',\n",
       " u'd',\n",
       " u'll',\n",
       " u'm',\n",
       " u'o',\n",
       " u're',\n",
       " u've',\n",
       " u'y',\n",
       " u'ain',\n",
       " u'aren',\n",
       " u'couldn',\n",
       " u'didn',\n",
       " u'doesn',\n",
       " u'hadn',\n",
       " u'hasn',\n",
       " u'haven',\n",
       " u'isn',\n",
       " u'ma',\n",
       " u'mightn',\n",
       " u'mustn',\n",
       " u'needn',\n",
       " u'shan',\n",
       " u'shouldn',\n",
       " u'wasn',\n",
       " u'weren',\n",
       " u'won',\n",
       " u'wouldn']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################\n",
    "### Exercise  ####\n",
    "##################\n",
    "\n",
    "\n",
    "# Create a variable called stemmed_stops which is the \n",
    "# stemmed version of each stopword in stopwords\n",
    "# Use the stemmer we used up above!\n",
    "\n",
    "stemmed_stops = [Stemmer.stem(word) for word in stopwords]\n",
    "stemmed_stops\n",
    "\n",
    "# Then create a list called stemmed_tokens_no_stop that \n",
    "# contains only the tokens in stemmed_tokens that aren't in \n",
    "# stemmed_stops\n",
    "\n",
    "\n",
    "# Show the 25 most common stemmed non stop word tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 5\n",
    "###Named Entity Recognition\n",
    "\n",
    "What:  Automatically extract the names of people, places, organizations, etc.\n",
    "\n",
    "Why:   Can help you to identify \"important\" words\n",
    "\n",
    "Notes: Training NER classifier requires a lot of annotated training data\n",
    "       Should be trained on data relevant to your task\n",
    "       Stanford NER classifier is the \"gold standard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ian', 'is', 'an', 'instructor', 'for', 'General', 'Assembly']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Ian is an instructor for General Assembly'\n",
    "\n",
    "tokenized = nltk.word_tokenize(sentence)\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ian', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('instructor', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('General', 'NNP'),\n",
       " ('Assembly', 'NNP')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource\n  u'chunkers/maxent_ne_chunker/english_ace_multiclass.pickle' not\n  found.  Please use the NLTK Downloader to obtain the resource:\n  >>> nltk.download()\n  Searched in:\n    - '/Users/samuelbolivar/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - u''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f0302b1b1ffa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelbolivar/anaconda/lib/python2.7/site-packages/nltk/chunk/__init__.pyc\u001b[0m in \u001b[0;36mne_chunk\u001b[0;34m(tagged_tokens, binary)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mchunker_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MULTICLASS_NE_CHUNKER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunker_pickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelbolivar/anaconda/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelbolivar/anaconda/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelbolivar/anaconda/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource\n  u'chunkers/maxent_ne_chunker/english_ace_multiclass.pickle' not\n  found.  Please use the NLTK Downloader to obtain the resource:\n  >>> nltk.download()\n  Searched in:\n    - '/Users/samuelbolivar/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - u''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "chunks = nltk.ne_chunk(tagged)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource\n  u'chunkers/maxent_ne_chunker/english_ace_multiclass.pickle' not\n  found.  Please use the NLTK Downloader to obtain the resource:\n  >>> nltk.download()\n  Searched in:\n    - '/Users/samuelbolivar/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - u''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-b9ec33af3753>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextract_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ian is an instructor for General Assembly'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'['\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'] '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-b9ec33af3753>\u001b[0m in \u001b[0;36mextract_entities\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# add part-of-speech tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# use NLTK's NER classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# parse the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mentities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchunk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelbolivar/anaconda/lib/python2.7/site-packages/nltk/chunk/__init__.pyc\u001b[0m in \u001b[0;36mne_chunk\u001b[0;34m(tagged_tokens, binary)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mchunker_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MULTICLASS_NE_CHUNKER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunker_pickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelbolivar/anaconda/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelbolivar/anaconda/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelbolivar/anaconda/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource\n  u'chunkers/maxent_ne_chunker/english_ace_multiclass.pickle' not\n  found.  Please use the NLTK Downloader to obtain the resource:\n  >>> nltk.download()\n  Searched in:\n    - '/Users/samuelbolivar/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - u''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "def extract_entities(text):\n",
    "    entities = []\n",
    "    # tokenize into sentences\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        # tokenize sentences into words\n",
    "        # add part-of-speech tags\n",
    "        # use NLTK's NER classifier\n",
    "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "        # parse the results\n",
    "        entities.extend([chunk for chunk in chunks if hasattr(chunk, 'label')])\n",
    "    return entities\n",
    "\n",
    "for entity in extract_entities('Ian is an instructor for General Assembly'):\n",
    "    print '[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 6\n",
    "###Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "What:  Computes \"relative frequency\" that a word appears in a document\n",
    "           compared to its frequency across all documents\n",
    "\n",
    "Why:   More useful than \"term frequency\" for identifying \"important\" words in\n",
    "           each document (high frequency in that document, low frequency in\n",
    "           other documents)\n",
    "\n",
    "Notes: Used for search engine scoring, text summarization, document clustering\n",
    "\n",
    "How: \n",
    "    TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "    IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = ['Bob likes sports', 'Bob hates sports', 'Bob likes likes trees']\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'bob', u'hates', u'likes', u'sports', u'trees']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row represents a sentence\n",
    "# Each column represents a word\n",
    "vect.fit_transform(sample).toarray()\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48133417,  0.        ,  0.61980538,  0.61980538,  0.        ],\n",
       "       [ 0.42544054,  0.72033345,  0.        ,  0.54783215,  0.        ],\n",
       "       [ 0.30861775,  0.        ,  0.7948031 ,  0.        ,  0.52253528]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(sample).toarray()\n",
    "#tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'bob': 1.0, u'trees': 1.6931471805599454, u'likes': 1.2876820724517808, u'hates': 1.6931471805599454, u'sports': 1.2876820724517808}\n"
     ]
    }
   ],
   "source": [
    "# the IDF of each word\n",
    "idf = tfidf.idf_\n",
    "print dict(zip(tfidf.get_feature_names(), idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############\n",
    "## Exercise ###\n",
    "###############\n",
    "\n",
    "\n",
    "# for each sentence in sample, find the most \"interesting \n",
    "#words\" by ordering their tfidf in ascending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 7\n",
    "\n",
    "###LDA - Latent Dirichlet Allocation\n",
    "\n",
    "What:  Way of automatically discovering topics from sentences\n",
    "\n",
    "Why:   Much quicker than manually creating and identifying topic clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-8be4fc5f024d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Instantiate a count vectorizer with two additional parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msentences_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "\n",
    "# Instantiate a count vectorizer with two additional parameters\n",
    "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \n",
    "sentences_train = vect.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "CountVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-2e2918eee40c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtopic_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_word_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_dist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtopic_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn_top_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Topic {}: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelbolivar/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;34m\"\"\"Array mapping from feature integer indices to feature name\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         return [t for t, i in sorted(six.iteritems(self.vocabulary_),\n",
      "\u001b[0;32m/Users/samuelbolivar/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocabulary_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelbolivar/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: CountVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "# Instantiate an LDA model\n",
    "model = lda.LDA(n_topics=10, n_iter=500)\n",
    "model.fit(sentences_train) # Fit the model \n",
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ', '.join(topic_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXAMPLE: Automatically summarize a document\n",
    "\n",
    "\n",
    "# corpus of 2000 movie reviews\n",
    "from nltk.corpus import movie_reviews\n",
    "reviews = [movie_reviews.raw(filename) for filename in movie_reviews.fileids()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create document-term matrix\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "dtm = tfidf.fit_transform(reviews)\n",
    "features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOWEST:\n",
      "\n",
      "where's joblo coming from ?\n",
      "he comes through as per his usual high-level style here , and gives the audience a completely different side to his talent , of which , i personally would like to see more .\n",
      "of course , we're receiving this lesson from a man who has put away a lot of scum in his time , but the things he seems to be doing aren't exactly on the up-and-up either .\n",
      "\n",
      "HIGHEST:\n",
      "\n",
      "the \n",
      "dirt on the streets , and proceeds to teach the rookie cop ( and us , the audience ) how things really work down there .\n",
      "it's a rookie cop's narcotics training alongside a seasoned police veteran .\n",
      "do you agree that you yourself would need to break a few small laws in order to put away the people who break the really big ones ?\n"
     ]
    }
   ],
   "source": [
    "# find the most and least \"interesting\" sentences in a randomly selected review\n",
    "def summarize():\n",
    "    \n",
    "    # choose a random movie review    \n",
    "    review_id = np.random.randint(0, len(reviews))\n",
    "    review_text = reviews[review_id]\n",
    "\n",
    "    # we are going to score each sentence in the review for \"interesting-ness\"\n",
    "    sent_scores = []\n",
    "    # tokenize document into sentences\n",
    "    for sentence in nltk.sent_tokenize(review_text):\n",
    "        # exclude short sentences\n",
    "        if len(sentence) > 6:\n",
    "            score = 0\n",
    "            token_count = 0\n",
    "            # tokenize sentence into words\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            # compute sentence \"score\" by summing TFIDF for each word\n",
    "            for token in tokens:\n",
    "                if token in features:\n",
    "                    score += dtm[review_id, features.index(token)]\n",
    "                    token_count += 1\n",
    "            # divide score by number of tokens\n",
    "            sent_scores.append((score / float(token_count + 1), sentence))\n",
    "\n",
    "    # lowest scoring sentences\n",
    "    print '\\nLOWEST:\\n'\n",
    "    for sent_score in sorted(sent_scores)[:3]:\n",
    "        print sent_score[1]\n",
    "\n",
    "    # highest scoring sentences\n",
    "    print '\\nHIGHEST:\\n'\n",
    "    for sent_score in sorted(sent_scores, reverse=True)[:3]:\n",
    "        print sent_score[1]\n",
    "\n",
    "# try it out!\n",
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): textblob in /Users/samuelbolivar/anaconda/lib/python2.7/site-packages\r\n",
      "Requirement already satisfied (use --upgrade to upgrade): nltk>=3.1 in /Users/samuelbolivar/anaconda/lib/python2.7/site-packages (from textblob)\r\n"
     ]
    }
   ],
   "source": [
    "# TextBlob Demo: \"Simplified Text Processing\"\n",
    "# Installation: pip install textblob\n",
    "! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['greg', 'adrian', u'general assembly'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify words and noun phrases\n",
    "blob = TextBlob('Greg and Adrian are instructors for General Assembly')\n",
    "blob.words\n",
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.9, -0.26923076923076916]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment analysis\n",
    "blob = TextBlob('I hate this horrible movie. This movie is not very good.')\n",
    "blob.sentences\n",
    "blob.sentiment.polarity\n",
    "[sent.sentiment.polarity for sent in blob.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# sentiment subjectivity\n",
    "TextBlob(\"I am a cool person\").sentiment.subjectivity # Pretty subjective\n",
    "TextBlob(\"I am a person\").sentiment.subjectivity # Pretty objective\n",
    "# different scores for essentially the same sentence\n",
    "print TextBlob('Greg and Adrian are instructors for General Assembly in Sydney').sentiment.subjectivity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Put', 'away', 'the', 'dish']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# singularize and pluralize\n",
    "blob = TextBlob('Put away the dishes.')\n",
    "[word.singularize() for word in blob.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Puts', 'aways', 'thes', 'dishess']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.pluralize() for word in blob.words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"15 minutes late\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spelling correction\n",
    "blob = TextBlob('15 minuets late')\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('part', 0.9929478138222849), (u'parrot', 0.007052186177715092)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spellcheck\n",
    "Word('parot').spellcheck()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'tip laterally',\n",
       " u'enclose with a bank',\n",
       " u'do business with a bank or keep an account at a bank',\n",
       " u'act as the banker in a game or in gambling',\n",
       " u'be in the banking business',\n",
       " u'put into a bank account',\n",
       " u'cover with ashes so to control the rate of burning',\n",
       " u'have confidence or faith in']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definitions\n",
    "Word('bank').define()\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'es'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translation and language identification\n",
    "blob = TextBlob('Welcome to the classroom.')\n",
    "blob.translate(to='es')\n",
    "blob = TextBlob('Hola amigos')\n",
    "blob.detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
